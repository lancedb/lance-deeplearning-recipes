{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKv-SOi56QDj"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z0okm88SLaVN"
      },
      "outputs": [],
      "source": [
        "import lance\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, CONFIG_MAPPING\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHDVqhKE6SQA"
      },
      "source": [
        "# Model, Hyperparameter and Tokenizer\n",
        "\n",
        "We'll be using the `wikitext_100K.lance` dataset that we created in an earlier example to train our GPT2 model from scratch.\n",
        "\n",
        "Change the `block_size` and `batch_size` based on your hardward and use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "95f-BNCuUgF7"
      },
      "outputs": [],
      "source": [
        "# Define necessary parameters\n",
        "model_name = 'gpt2'\n",
        "lr = 3e-4\n",
        "nb_epochs = 2\n",
        "block_size = 1024\n",
        "batch_size = 8\n",
        "device = 'cuda:0'\n",
        "dataset_path = 'wikitext_100K.lance' # the dataset we created in a previous example\n",
        "\n",
        "\n",
        "# We'll be training the a GPT2 model from scratch in this example for a couple epochs\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "config = CONFIG_MAPPING[model_name]()\n",
        "model = AutoModelForCausalLM.from_config(config, trust_remote_code=True).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nuj24Fqm6lFu"
      },
      "source": [
        "# Custom Dataset and Sampler\n",
        "\n",
        "The dataset that we created in our previous example is essentially a long, contiguous table of tokens.\n",
        "\n",
        "We need to retrieve a causal window of tokens of `block_size` amount **and** make sure that window doesn't overlap with any other window of tokens.\n",
        "\n",
        "The most elegant way to achieve this is to make a custom sampler that only samples random indices that are `block_size` or more apart. This way our windows of tokens will actually behave as as individual samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6blBtbXtb6H3"
      },
      "outputs": [],
      "source": [
        "def from_indices(dataset, indices):\n",
        "    \"\"\"Load the elements on given indices from the dataset\"\"\"\n",
        "    chunk = dataset.take(indices).to_pylist()\n",
        "    chunk = list(map(lambda x: x['input_ids'], chunk))\n",
        "    return chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WvC8n_MCb9MA"
      },
      "outputs": [],
      "source": [
        "class LanceDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_path,\n",
        "        block_size,\n",
        "    ):\n",
        "        # Load the lance dataset from the saved path\n",
        "        self.ds = lance.dataset(dataset_path)\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # Doing this so the sampler never asks for an index at the end of text\n",
        "        self.length = self.ds.count_rows() - block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Generate a window of indices starting from the current idx to idx+block_size\n",
        "        and return the tokens at those indices\n",
        "        \"\"\"\n",
        "        window = np.arange(idx, idx + self.block_size)\n",
        "        sample = from_indices(self.ds, window)\n",
        "\n",
        "        return {\"input_ids\": torch.tensor(sample), \"labels\": torch.tensor(sample)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vEzLIV0qb-hZ"
      },
      "outputs": [],
      "source": [
        "class LanceSampler(Sampler):\n",
        "    r\"\"\"Samples tokens randomly but `block_size` indices apart to emulate unique samples\n",
        "\n",
        "    Args:\n",
        "        data_source (Dataset): dataset to sample from\n",
        "        block_size (int): minimum index distance between each random sample\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_source, block_size=1024):\n",
        "        self.data_source = data_source\n",
        "        self.num_samples = len(self.data_source)\n",
        "        self.available_indices = list(range(0, self.num_samples, block_size))\n",
        "        np.random.shuffle(self.available_indices)\n",
        "\n",
        "    def __iter__(self):\n",
        "        yield from self.available_indices\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.available_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pV7tACM7Wbx"
      },
      "source": [
        "# Train!\n",
        "\n",
        "After this, the model training is pretty standard. One has to load the batch from the dataloader, transfer all it's elements (input_ids and labels) to GPU, pass them through the model, run backward pass and optimize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejstV6-ycBRh",
        "outputId": "56d17258-72bd-4a9f-8665-2a557d31f3e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========= Epoch: 1 / 2 =========\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss: 5.4288: 100%|██████████| 1222/1222 [11:57<00:00,  1.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_perplexity: 428.88519977329736\n",
            "========= Epoch: 2 / 2 =========\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss: 4.6657: 100%|██████████| 1222/1222 [11:56<00:00,  1.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_perplexity: 139.86871656813784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the dataset, sampler and dataloader\n",
        "dataset = LanceDataset(dataset_path, block_size)\n",
        "sampler = LanceSampler(dataset, block_size)\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size,\n",
        "    sampler=sampler,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Define the optimizer, training loop and train the model!\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    print(f\"========= Epoch: {epoch+1} / {nb_epochs} =========\")\n",
        "    epoch_loss = []\n",
        "    prog_bar = tqdm(dataloader, total=len(dataloader))\n",
        "    for batch in prog_bar:\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Put both input_ids and labels to the device\n",
        "        for k, v in batch.items():\n",
        "            batch[k] = v.to(device)\n",
        "\n",
        "        # Perform one forward pass and get the loss\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Perform backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "    # Calculate training perplexity for this epoch\n",
        "    try:\n",
        "        perplexity = np.exp(np.mean(epoch_loss))\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"-inf\")\n",
        "\n",
        "    print(f\"train_perplexity: {perplexity}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is a very minimal training notebook that will show you how to fine-tune a `resnet50` model for Image segmentation task (given an image, we predict it's mask).\n",
    "\n",
    "The point of this notebook is not to serve as a complete end-to-end training example, but to serve a reference on how to write training scripts for Image-based Lance datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import lance\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets, tv_tensors\n",
    "import torchvision.models.segmentation as models\n",
    "\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function\n",
    "\n",
    "Below are some utility functions to make mask using the mask coordinates and to resize the mask and image together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_list(input_dict):\n",
    "    keys = list(input_dict.keys())\n",
    "    values = list(input_dict.values())\n",
    "    \n",
    "    result = [dict(zip(keys, sublist)) for sublist in zip(*values)]\n",
    "    return result\n",
    "\n",
    "def get_mask(img, segmentation, category):\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    if type(segmentation) == list:\n",
    "        rles = maskUtils.frPyObjects(segmentation, h, w)\n",
    "        rle = maskUtils.merge(rles)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown annotation type. Expected list, recieved '{type(segmentation)}'\")\n",
    "\n",
    "    m = maskUtils.decode(rle)\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    mask[:, :] += (mask == 0) * (m * category)\n",
    "\n",
    "    return torch.from_numpy(mask)\n",
    "\n",
    "def resize_image_mask(image, mask, size):\n",
    "    # Resize the image using bilinear interpolation\n",
    "    resized_image = torch.nn.functional.interpolate(image.permute(2, 0, 1).unsqueeze(0), size=size, mode='bilinear', align_corners=False)[0]\n",
    "    \n",
    "    # Resize the mask using nearest-neighbor interpolation\n",
    "    resized_mask = torch.nn.functional.interpolate(mask.unsqueeze(0).unsqueeze(0), size=size, mode='nearest')[0, 0]\n",
    "    \n",
    "    return resized_image, resized_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCOLanceDataset\n",
    "\n",
    "This is the Custom Dataset that allows to loads COCO2017 Lance dataset.\n",
    "\n",
    "We pre-load the annotations to match what PyTorch's COCO dataloader (by loading in the annotations JSON file). This makes the notebook comparable to equivalent-PyTorch's COCO dataloader. However, in case you don't want this functionality, you can remove it completely and replace it with the logic to fetch the annotations from the Lance dataset (similar how we are fetching the images).\n",
    "\n",
    "In this dataset we load the images and the segmentation coordinates from the lance dataset and then we generate a mask using the `pycocoutils` package and resize both the image and mask to be of same heigh and width (so they can be stacked by PyTorch's default data collator, if you don't want it, use a custom data collator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOLanceDataset(Dataset):\n",
    "    def __init__(self, dataset_path, resize=(600, 600), transforms=None):\n",
    "        self.ds = lance.dataset(dataset_path)\n",
    "        self.resize = resize\n",
    "        self.transforms = transforms\n",
    "        self.annotations = self.preload_annotations()\n",
    "\n",
    "    def preload_annotations(self, exclude=['image', 'width', 'height', 'image_path']):\n",
    "        \"\"\"\n",
    "        This will load all the annotations in memory which speeds up operations \n",
    "        drastically during the training but adds some overhead cost during init\n",
    "        \"\"\"\n",
    "        print(\"Preloading all annotations in the memory.\")\n",
    "        idxs = [x for x in range(self.ds.count_rows())]\n",
    "        cols = list(self.ds.take([0]).to_pydict().keys())\n",
    "        cols = [col for col in cols if col not in exclude]\n",
    "        return self.ds.take(idxs, columns=cols).to_pylist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.count_rows()\n",
    "\n",
    "    def _load_image(self, idx):\n",
    "        raw_img = self.ds.take([idx], columns=['image']).to_pydict()\n",
    "        raw_img = np.frombuffer(b''.join(raw_img['image']), dtype=np.uint8)\n",
    "        img = cv2.imdecode(raw_img, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        return torch.from_numpy(img)\n",
    "\n",
    "    def _load_target(self, idx):\n",
    "        return self.annotations[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self._load_image(idx)\n",
    "        raw_anns = self._load_target(idx)\n",
    "\n",
    "        anns = dict(\n",
    "            image_id=[raw_anns['image_id']]*len(raw_anns['bbox']),\n",
    "            segmentation=raw_anns['segmentation'], \n",
    "            bbox=raw_anns['bbox'], \n",
    "            area=raw_anns['area'], \n",
    "            iscrowd=raw_anns['is_crowd'],\n",
    "            category_id=raw_anns['category_id']\n",
    "        )\n",
    "        \n",
    "        anns = convert_dict_to_list(anns)\n",
    "\n",
    "        # Get only the first mask for the image for the sake of demonstration\n",
    "        mask = get_mask(image, anns[0]['segmentation'], anns[0]['category_id'])\n",
    "\n",
    "        # Resize both image and mask accordingly\n",
    "        image, mask = resize_image_mask(image, mask, size=self.resize)\n",
    "\n",
    "        # transforms\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image.float())\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "This training part of this notebook is fairly straightforward, we begin by defining our dataset, a very normalisation transformation and then defining the dataloader.\n",
    "\n",
    "Once that is done and the annotations are pre-loaded in the memory (which takes a little while because there are 100K+ of them!), we define a model, loss function and optimizer and then train the model.\n",
    "\n",
    "In the training logs below, you will see the reported per-batch time (to train) and total time (to train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading all annotations in the memory.\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset and the dataloader\n",
    "ds = COCOLanceDataset(\n",
    "    dataset_path='coco2017_train_lance/coco2017_train_new.lance/', \n",
    "    transforms=transforms.Compose([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    ")\n",
    "\n",
    "dl = DataLoader(\n",
    "    ds,\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes=134):\n",
    "    model = models.deeplabv3_resnet50(pretrained=True)\n",
    "    model.classifier[4] = torch.nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.1363:  99%|█████████▉| 127/128 [01:59<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (128 batches) | Loss: 1.3862293751444668 | Avg Per-batch time: 0.9303 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.0304:  99%|█████████▉| 127/128 [01:58<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (128 batches) | Loss: 0.7514677769504488 | Avg Per-batch time: 0.9258 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.9979:  99%|█████████▉| 127/128 [01:58<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 (128 batches) | Loss: 0.735734045621939 | Avg Per-batch time: 0.9240 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.0773:  99%|█████████▉| 127/128 [01:58<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 (128 batches) | Loss: 0.726858394453302 | Avg Per-batch time: 0.9255 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.9969:  99%|█████████▉| 127/128 [01:58<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 (128 batches) | Loss: 0.7257018785458058 | Avg Per-batch time: 0.9239 seconds\n",
      "Total time taken for 5 epochs and 128 batches: 9.8766 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_model().to(\"cuda:0\")\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=5, batch_to_train=128):\n",
    "    model.train()\n",
    "    total_start = time.time()\n",
    "    loss = torch.tensor([0])\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(enumerate(dataloader), total=batch_to_train)\n",
    "        total_batch_start = time.time()\n",
    "        \n",
    "        for idx, (images, masks) in pbar:\n",
    "            if idx+1 == batch_to_train:\n",
    "                break\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images.to(\"cuda:0\"))['out']\n",
    "            loss = criterion(outputs, masks.long().to(\"cuda:0\"))\n",
    "            \n",
    "            pbar.set_description(f\"loss: {loss.item():.4f}\")\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        per_batch_time = (time.time() - total_batch_start) / batch_to_train\n",
    "\n",
    "        print(f'Epoch {epoch+1} ({batch_to_train} batches) | Loss: {running_loss/batch_to_train} | Avg Per-batch time: {per_batch_time:.4f} seconds')\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"Total time taken for {num_epochs} epochs and {batch_to_train} batches: {total_time/60:.4f} mins\")\n",
    "\n",
    "train_model(model, dl, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

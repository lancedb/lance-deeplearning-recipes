{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Setup\n",
        "\n",
        "Setup here below assumes your `kaggle.json` key has been uploaded to the Colab before execute the below cell.\n",
        "\n",
        "It installs all dependencies not previously available in Google Colab, downloads and unzips the `flickr8k.lance` dataset."
      ],
      "metadata": {
        "id": "IQvtSmxJl4c8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkAtBe9j3ikE",
        "outputId": "7a2e3a93-400c-403e-e0eb-3b34f023affe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.4/25.4 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/heyytanay/flickr-8k-lance\n",
            "License(s): CC0-1.0\n",
            "Downloading flickr-8k-lance.zip to /content\n",
            "100% 1.03G/1.04G [00:49<00:00, 23.3MB/s]\n",
            "100% 1.04G/1.04G [00:49<00:00, 22.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "! mkdir /root/.kaggle/\n",
        "! pip install -q pyarrow pylance transformers kaggle timm\n",
        "! mv /content/kaggle.json /root/.kaggle/\n",
        "! chmod +rwx /root/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d heyytanay/flickr-8k-lance\n",
        "! unzip -qq flickr-8k-lance.zip\n",
        "! rm flickr-8k-lance.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import lance\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import timm\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "qA29F_fO4IBK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config, Utility and Transformations\n",
        "\n",
        "Define a Config with hyperparameters, some utility functions to load the image and captions and image transformations to pre-process the images."
      ],
      "metadata": {
        "id": "DaxIkUclmMxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    img_size = (128, 128)\n",
        "    bs = 32\n",
        "    head_lr = 1e-3\n",
        "    img_enc_lr = 1e-4\n",
        "    text_enc_lr = 1e-5\n",
        "    max_len = 18\n",
        "    img_embed_dim = 2048\n",
        "    text_embed_dim = 768\n",
        "    projection_dim = 256\n",
        "    temperature = 1.0\n",
        "    num_epochs = 2\n",
        "    img_encoder_model = 'resnet50'\n",
        "    text_encoder_model = 'bert-base-cased'"
      ],
      "metadata": {
        "id": "9pyGnUPXGQKk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(ds, idx):\n",
        "    # Utility function to load an image at an index and convert it from bytes format to img format\n",
        "    raw_img = ds.take([idx], columns=['image']).to_pydict()\n",
        "    raw_img = np.frombuffer(b''.join(raw_img['image']), dtype=np.uint8)\n",
        "    img = cv2.imdecode(raw_img, cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    return img\n",
        "\n",
        "def load_caption(ds, idx):\n",
        "    # Utility function to load an image's caption. Currently we return the longest caption of all\n",
        "    captions = ds.take([idx], columns=['captions']).to_pydict()['captions'][0]\n",
        "    return max(captions, key=len)"
      ],
      "metadata": {
        "id": "hucXkHOm6dxI"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_augments = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize(Config.img_size),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "r_AeGE2jAxmf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLIPLanceDataset\n",
        "\n",
        "This is a custom pytorch dataset for loading the Flickr8k images and captions from the lance dataset, tokenizing / pre-processing them and then returning them in the proper format."
      ],
      "metadata": {
        "id": "BZVduXHnmZFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPLanceDataset(Dataset):\n",
        "    \"\"\"Custom Dataset to load images and their corresponding captions\"\"\"\n",
        "    def __init__(self, lance_path, max_len=18, tokenizer=None, transforms=None):\n",
        "        self.ds = lance.dataset(lance_path)\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-cased') if not tokenizer else tokenizer\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.ds.count_rows()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the image\n",
        "        img = load_image(self.ds, idx)\n",
        "        caption = load_caption(self.ds, idx)\n",
        "\n",
        "        # Tokenize the caption\n",
        "        caption = self.tokenizer(\n",
        "            caption,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        # Flatten the tokenized captions otherwise they cause errors during training\n",
        "        for k, v in caption.items():\n",
        "            caption[k] = v.flatten()\n",
        "\n",
        "        # Apply transformations to the images\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, caption"
      ],
      "metadata": {
        "id": "wn3XMA3m6h7a"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "The Image Encoder, Text Encoder and the whole model architecture in general was adapted and inspired from Manan Goel's work, [Implementing CLIP with PyTorch Lightning](https://wandb.ai/manan-goel/coco-clip/reports/Implementing-CLIP-With-PyTorch-Lightning--VmlldzoyMzg4Njk1).\n",
        "\n",
        "This specific implementation is for CLIP for natural language-based image search.\n",
        "\n",
        "The part that is of importance to us in this example is how the data loading works which can be found in the earlier sample and can be used as a drop-in replacement for many use-cases."
      ],
      "metadata": {
        "id": "e5VwKxA6mrlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, model_name, pretrained = True):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            num_classes=0,\n",
        "            global_pool=\"avg\"\n",
        "        )\n",
        "\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.backbone(img)\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, captions):\n",
        "        output = self.backbone(**captions)\n",
        "        return output.last_hidden_state[:, 0, :]\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, embedding_dim, projection_dim):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        projected = self.projection(x)\n",
        "        x = self.gelu(projected)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x += projected\n",
        "\n",
        "        return self.layer_norm(x)"
      ],
      "metadata": {
        "id": "Hqh6bUNyG5T-"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "We finally now define the model, optimizer, dataloader and train the model for 2 epochs."
      ],
      "metadata": {
        "id": "UtwpJ8Qznmdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models, optimizer, etc\n",
        "img_encoder = ImageEncoder(model_name=Config.img_encoder_model).to('cuda')\n",
        "img_head = Head(Config.img_embed_dim, Config.projection_dim).to('cuda')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(Config.text_encoder_model)\n",
        "text_encoder = TextEncoder(model_name=Config.text_encoder_model).to('cuda')\n",
        "text_head = Head(Config.text_embed_dim, Config.projection_dim).to('cuda')\n",
        "\n",
        "parameters = [\n",
        "    {\"params\": img_encoder.parameters(), \"lr\": Config.img_enc_lr},\n",
        "    {\"params\": text_encoder.parameters(), \"lr\": Config.text_enc_lr},\n",
        "    {\n",
        "        \"params\": itertools.chain(\n",
        "            img_head.parameters(),\n",
        "            text_head.parameters(),\n",
        "        ),\n",
        "        \"lr\": Config.head_lr,\n",
        "    },\n",
        "]\n",
        "\n",
        "optimizer = torch.optim.Adam(parameters)"
      ],
      "metadata": {
        "id": "q2ceQ1jkMLtk"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some utilities to simplify the training."
      ],
      "metadata": {
        "id": "XBKHJqcPnzJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(img_embed, text_embed, temperature=0.2):\n",
        "    logits = (text_embed @ img_embed.T) / temperature\n",
        "    img_sim = img_embed @ img_embed.T\n",
        "    text_sim = text_embed @ text_embed.T\n",
        "    targets = F.softmax(\n",
        "        (img_sim + text_sim) / 2 * temperature, dim=-1\n",
        "    )\n",
        "    img_loss = (-targets.T * nn.LogSoftmax(dim=-1)(logits.T)).sum(1)\n",
        "    text_loss = (-targets * nn.LogSoftmax(dim=-1)(logits)).sum(1)\n",
        "    return (img_loss + text_loss) / 2.0\n",
        "\n",
        "def forward(img, caption):\n",
        "    # Transfer to device\n",
        "    img = img.to('cuda')\n",
        "    for k, v in caption.items():\n",
        "        caption[k] = v.to('cuda')\n",
        "\n",
        "    # Get embeddings for both img and caption\n",
        "    img_embed = img_head(img_encoder(img))\n",
        "    text_embed = text_head(text_encoder(caption))\n",
        "\n",
        "    return img_embed, text_embed"
      ],
      "metadata": {
        "id": "8xlCEXTvUQZs"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CLIPLanceDataset(\n",
        "    lance_path=\"flickr8k.lance\",\n",
        "    max_len=Config.max_len,\n",
        "    tokenizer=tokenizer,\n",
        "    transforms=train_augments\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    shuffle=False,\n",
        "    batch_size=Config.bs,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "oE4S58drALjc"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train!\n",
        "img_encoder.train()\n",
        "img_head.train()\n",
        "text_encoder.train()\n",
        "text_head.train()\n",
        "\n",
        "for epoch in range(Config.num_epochs):\n",
        "    print(f\"{'='*20} Epoch: {epoch+1} / {Config.num_epochs} {'='*20}\")\n",
        "\n",
        "    prog_bar = tqdm(dataloader)\n",
        "    for img, caption in prog_bar:\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        img_embed, text_embed = forward(img, caption)\n",
        "        loss = loss_fn(img_embed, text_embed, temperature=Config.temperature).mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyKCOrvFQy8D",
        "outputId": "c99332c7-8c39-419b-dbf0-68f4466abd0b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================== Epoch: 1 / 2 ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 2.0799: 100%|██████████| 253/253 [02:14<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== Epoch: 2 / 2 ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 1.3064: 100%|██████████| 253/253 [02:10<00:00,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BLDA6tKEihw5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
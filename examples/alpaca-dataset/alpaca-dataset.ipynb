{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0be23a8a-e8e3-403b-b126-bc67414d49ac",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde1d753-1704-446e-a79e-d74aa9597e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lance\n",
    "import pyarrow as pa\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0761ca8-f47b-450c-abd8-2766a78b89bf",
   "metadata": {},
   "source": [
    "# Tokenizer and Dataset\n",
    "\n",
    "We are using the alpaca instruction dataset in this example walkthrough. We'll be splitting the dataset into train and validation splits. Both splits will be saved as separate lance datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47569cb-faf1-4122-8175-13315f63494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "split = 0.10 # We'll use 10% data for validation set\n",
    "\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\").shuffle(seed=42)\n",
    "dataset['val'] = load_dataset(\"tatsu-lab/alpaca\", split=f\"train[:{int(split * 100)}%]\").shuffle(seed=42)\n",
    "dataset['train'] = load_dataset(\"tatsu-lab/alpaca\", split=f\"train[{int(split * 100)}%:]\").shuffle(seed=42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a73aaf-2c91-425e-913c-1a5847d7f5da",
   "metadata": {},
   "source": [
    "# Processing Samples\n",
    "\n",
    "This is the most important function here. We go over each sample in the ðŸ¤— dataset, tokenize all the column-values in that sample and then yield a pyarrow `RecordBatch` consisting of the tokens we just tokenized.\n",
    "\n",
    "Each sample from the original dataset will be stored as a row in the pyarrow table (unlike the llm-pretraining dataset, where all the samples were stored in one large contiguous row of tokens).\n",
    "\n",
    "This will make it considerably easier to iterate over them during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117e8bde-e515-480d-97e3-d2d7821f0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(dataset, tokenizer):\n",
    "    for sample in tqdm(dataset):\n",
    "        inst, inp, outp, text = sample['instruction'], sample['input'], sample['output'], sample['text']\n",
    "        \n",
    "        # There are empty strings present in the dataset which we are ignoring\n",
    "        if not (inst and inp and outp and text):\n",
    "            continue\n",
    "\n",
    "        # Tokenize all the text data\n",
    "        inst = tokenizer(inst)['input_ids']\n",
    "        inp = tokenizer(inp)['input_ids']\n",
    "        outp = tokenizer(outp)['input_ids']\n",
    "        text = tokenizer(text)['input_ids']\n",
    "        \n",
    "        # Return a Pyarrow record batch with all the tokenized data\n",
    "        yield pa.RecordBatch.from_arrays(\n",
    "            [\n",
    "                pa.array([inst], pa.list_(pa.int64(), -1)),\n",
    "                pa.array([inp], pa.list_(pa.int64(), -1)),\n",
    "                pa.array([outp], pa.list_(pa.int64(), -1)),\n",
    "                pa.array([text], pa.list_(pa.int64(), -1)),\n",
    "            ],\n",
    "            [\"instructions\", \"inputs\", \"outputs\", \"texts\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc138b-5485-462a-b1f0-f141e3c3ae84",
   "metadata": {},
   "source": [
    "# Writing the dataset to disk\n",
    "\n",
    "Now that our processing function is ready, we define a schema that tells pyarrow what format of data it should be expecting in the table and we define reader functions (for both train and val splits) that will take in the schema and an iterator (or a function) which will yield the RecordBatches.\n",
    "\n",
    "Finally, we use those reader functions by calling `lance.write_dataset` to write these pyarrow tables to disk in the highly efficient and fast, lance file format.\n",
    "\n",
    "That's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91c0d48-e27b-4990-8d01-53f174765e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema to tell pyarrow the type of data we are expecting in our table\n",
    "schema = pa.schema([\n",
    "    pa.field(\"instructions\", pa.list_(pa.int64(), -1)),\n",
    "    pa.field(\"inputs\", pa.list_(pa.int64(), -1)),\n",
    "    pa.field(\"outputs\", pa.list_(pa.int64(), -1)),\n",
    "    pa.field(\"texts\", pa.list_(pa.int64(), -1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f0a82ac-6fcb-44a2-9828-d53c992141e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46802/46802 [00:15<00:00, 3081.87it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5200/5200 [00:01<00:00, 2914.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lance.dataset.LanceDataset at 0x7feb91b24880>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These will be used by lance to write the dataset\n",
    "train_reader = pa.RecordBatchReader.from_batches(schema, process(dataset['train'], tokenizer))\n",
    "val_reader = pa.RecordBatchReader.from_batches(schema, process(dataset['val'], tokenizer))\n",
    "\n",
    "# Write the train and val datasets to disk\n",
    "lance.write_dataset(\n",
    "    train_reader,\n",
    "    \"alpaca_train.lance\",\n",
    "    schema\n",
    ")\n",
    "lance.write_dataset(\n",
    "    val_reader,\n",
    "    \"alpaca_val.lance\",\n",
    "    schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e861c7db-b48b-4201-9904-a6b3ff3081aa",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "\n",
    "Let's load our newly created dataset and see how many samples we have in our dataset.\n",
    "\n",
    "You might notice a significantly lower number of samples in our newly created training set below. This is due to the fact that many columns have empty strings which would raise an error not just with the tokenizer but also with pyarrow and hence we are not including those rows altogether.\n",
    "\n",
    "If your use-case allows, you can always assign a placeholder token or any other string in place of an empty string to keep all the samples intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9449ced3-39d9-4ae7-beb8-996d3f1fe74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in alpaca-train set: 18,385\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset to inspect the total number of samples\n",
    "ds = lance.dataset('alpaca_train.lance')\n",
    "print(f\"Total samples in alpaca-train set: {ds.count_rows():,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435684c8-0a50-4b44-8c30-7c33abce1c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
